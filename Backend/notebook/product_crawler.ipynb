{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b576539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 14:34:03,832 [INFO] Starting crawl: https://www.netcomlearning.com/ (max 100 pages)\n",
      "Pages crawled:   1%|          | 1/100 [00:03<05:33,  3.37s/it]\n",
      "2025-09-22 14:34:19,428 [INFO] Crawl finished. Pages visited: 1. Products found: 0\n",
      "2025-09-22 14:34:19,428 [INFO] Wrote products to product.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AI-enabled link crawler (improved link discovery + Playwright interactions)\n",
    "\n",
    "Usage:\n",
    "  python ai_crawler_full.py --start-url https://www.netcomlearning.com/ --output netcom_links.json --max-pages 200 --concurrency 2\n",
    "\n",
    "Notes:\n",
    "- Install dependencies:\n",
    "    pip install playwright requests beautifulsoup4\n",
    "    playwright install\n",
    "- OpenAI integration is optional and disabled if OPENAI_API_KEY is not set.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse, urljoin, urlunparse, parse_qsl\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser as robotparser\n",
    "\n",
    "# Playwright\n",
    "try:\n",
    "    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "except Exception:\n",
    "    async_playwright = None\n",
    "    PlaywrightTimeoutError = Exception\n",
    "\n",
    "# Optional OpenAI (modern client)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "\n",
    "# Logging setup\n",
    "logger = logging.getLogger('ai_crawler_full')\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Constants\n",
    "USER_AGENT = 'AICompanyLinkCrawler/1.0 (+https://example.com)'\n",
    "HEADERS = {'User-Agent': USER_AGENT}\n",
    "EXTENSION_SKIP = ('.jpg', '.jpeg', '.png', '.gif', '.svg', '.pdf', '.zip', '.rar', '.exe', '.tar', '.gz')\n",
    "COMMON_MENU_SELECTORS = ['.menu', '.nav', '.dropdown', '[data-toggle]', '[aria-haspopup]']\n",
    "\n",
    "# Optional OpenAI setup (if available)\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "OPENAI_MODEL = os.environ.get('OPENAI_MODEL', 'gpt-4o-mini')\n",
    "openai_client = None\n",
    "if OpenAI is not None and OPENAI_API_KEY:\n",
    "    try:\n",
    "        openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        logger.info('OpenAI client initialized (optional features enabled)')\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Failed to init OpenAI client: {e}')\n",
    "        openai_client = None\n",
    "else:\n",
    "    logger.info('OpenAI client not available/disabled; using heuristics only')\n",
    "\n",
    "# Helper: normalize urls to deduplicate\n",
    "def normalize_url(u, base=None):\n",
    "    if not u:\n",
    "        return None\n",
    "    if base:\n",
    "        u = urljoin(base, u)\n",
    "    # strip fragments\n",
    "    u = u.split('#', 1)[0]\n",
    "    parsed = urlparse(u)\n",
    "    scheme = (parsed.scheme or 'http').lower()\n",
    "    netloc = parsed.netloc.lower()\n",
    "    path = parsed.path or '/'\n",
    "    # normalize path: remove trailing slash except root\n",
    "    if path != '/' and path.endswith('/'):\n",
    "        path = path.rstrip('/')\n",
    "    # remove common tracking params\n",
    "    qs_items = parse_qsl(parsed.query, keep_blank_values=True)\n",
    "    qs_filtered = [(k, v) for k, v in qs_items if not (k.lower().startswith('utm_') or k.lower() in ('fbclid', 'gclid', 'icid'))]\n",
    "    query = '&'.join(f'{k}={v}' for k, v in qs_filtered) if qs_filtered else ''\n",
    "    normalized = urlunparse((scheme, netloc, path, '', query, ''))\n",
    "    return normalized\n",
    "\n",
    "# Plain fetch with requests (runs in thread)\n",
    "def fetch_plain(url, timeout=15):\n",
    "    logger.debug(f'fetch_plain start: {url}')\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        logger.debug(f'fetch_plain success: {url} (len={len(r.text)})')\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        logger.debug(f'fetch_plain failed {url}: {e}')\n",
    "        return None\n",
    "\n",
    "# Playwright fetch with additional JS-based link collection and optional hover attempts\n",
    "async def fetch_with_playwright(context, url, wait_until='networkidle', timeout=30000):\n",
    "    logger.debug(f'fetch_with_playwright start: {url}')\n",
    "    page = await context.new_page()\n",
    "    try:\n",
    "        await page.set_extra_http_headers({\"User-Agent\": USER_AGENT})\n",
    "        # goto\n",
    "        await page.goto(url, wait_until=wait_until, timeout=timeout)\n",
    "        # short wait for dynamic content\n",
    "        await asyncio.sleep(0.35)\n",
    "\n",
    "        # Try hovering common menu selectors to reveal hidden links\n",
    "        for sel in COMMON_MENU_SELECTORS:\n",
    "            try:\n",
    "                # try a few times: if selector exists, hover first matching element\n",
    "                if await page.query_selector(sel):\n",
    "                    try:\n",
    "                        await page.hover(sel)\n",
    "                        logger.debug(f'hovered selector {sel} on {url}')\n",
    "                        await asyncio.sleep(0.15)\n",
    "                    except Exception:\n",
    "                        # sometimes hover fails; ignore\n",
    "                        logger.debug(f'hover failed for selector {sel} on {url}')\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Evaluate JS to collect additional link candidates created by scripts, data attributes, onclick handlers\n",
    "        js_collect = r\"\"\"\n",
    "        () => {\n",
    "          const urls = new Set();\n",
    "          function add(u){ if(u && typeof u === 'string') urls.add(u); }\n",
    "          // anchors\n",
    "          for(const a of document.querySelectorAll('a[href]')) add(a.getAttribute('href'));\n",
    "          // data attributes often used by SPA menus\n",
    "          const dataAttrs = ['data-href','data-url','data-link','data-target'];\n",
    "          for(const attr of dataAttrs){\n",
    "            for(const el of document.querySelectorAll('['+attr+']')){\n",
    "              add(el.getAttribute(attr));\n",
    "            }\n",
    "          }\n",
    "          // onclick patterns\n",
    "          for(const el of document.querySelectorAll('[onclick]')){\n",
    "            const s = el.getAttribute('onclick') || '';\n",
    "            let m = s.match(/location\\.href\\s*=\\s*['\"]([^'\"]+)['\"]/);\n",
    "            if(m) add(m[1]);\n",
    "            m = s.match(/window\\.location(?:\\.href)?\\s*=\\s*['\"]([^'\"]+)['\"]/);\n",
    "            if(m) add(m[1]);\n",
    "            m = s.match(/window\\.open\\(\\s*['\"]([^'\"]+)['\"]/);\n",
    "            if(m) add(m[1]);\n",
    "          }\n",
    "          // attributes that may contain urls\n",
    "          for(const el of document.querySelectorAll('[href], [src], [data-href], [data-url]')){\n",
    "            for(const k of ['href','src','data-href','data-url']) {\n",
    "              if(el.getAttribute(k)) add(el.getAttribute(k));\n",
    "            }\n",
    "          }\n",
    "          // meta/link rel canonical/alternate\n",
    "          for(const link of document.querySelectorAll('link[href], meta[content]')) {\n",
    "            const rel = link.getAttribute('rel') || '';\n",
    "            const href = link.getAttribute('href') || link.getAttribute('content') || '';\n",
    "            if(rel && ['canonical','prev','next','alternate'].some(r=>rel.includes(r))) add(href);\n",
    "          }\n",
    "          return Array.from(urls);\n",
    "        }\n",
    "        \"\"\"\n",
    "        collected = await page.evaluate(js_collect)\n",
    "        # page content after possible hover\n",
    "        content = await page.content()\n",
    "\n",
    "        # close page\n",
    "        try:\n",
    "            await page.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Return tuple: html + extra links found by JS\n",
    "        logger.debug(f'fetch_with_playwright success: {url} (len={len(content)}), collected {len(collected)} extra links')\n",
    "        return content, collected\n",
    "    except PlaywrightTimeoutError:\n",
    "        logger.debug(f'PlaywrightTimeoutError for {url}')\n",
    "        try:\n",
    "            await page.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None, []\n",
    "    except Exception as e:\n",
    "        logger.debug(f'Playwright fetch error {url}: {e}')\n",
    "        try:\n",
    "            await page.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None, []\n",
    "\n",
    "# Enhanced link extraction and normalization\n",
    "def extract_links_from_html(base_url, html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    found = set()\n",
    "\n",
    "    # 1) anchors\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href'].strip()\n",
    "        if href:\n",
    "            n = normalize_url(href, base=base_url)\n",
    "            if n:\n",
    "                found.add(n)\n",
    "\n",
    "    # 2) data attributes commonly used by JS\n",
    "    for attr in ('data-href', 'data-url', 'data-link', 'data-target'):\n",
    "        for el in soup.find_all(attrs={attr: True}):\n",
    "            v = el.get(attr)\n",
    "            if v:\n",
    "                n = normalize_url(v, base=base_url)\n",
    "                if n:\n",
    "                    found.add(n)\n",
    "\n",
    "    # 3) onclick patterns\n",
    "    for el in soup.find_all(attrs={'onclick': True}):\n",
    "        onclick = el.get('onclick') or ''\n",
    "        m = re.search(r\"\"\"location\\.href\\s*=\\s*['\"]([^'\"]+)['\"]\"\"\", onclick)\n",
    "        if not m:\n",
    "            m = re.search(r\"\"\"window\\.location(?:\\.href)?\\s*=\\s*['\"]([^'\"]+)['\"]\"\"\", onclick)\n",
    "        if not m:\n",
    "            m = re.search(r\"\"\"window\\.open\\(\\s*['\"]([^'\"]+)['\"]\"\"\", onclick)\n",
    "        if m:\n",
    "            n = normalize_url(m.group(1), base=base_url)\n",
    "            if n:\n",
    "                found.add(n)\n",
    "\n",
    "    # 4) head link rel canonical/alternate\n",
    "    for tag in soup.find_all('link', href=True):\n",
    "        rel = tag.get('rel') or []\n",
    "        if isinstance(rel, list) and any(r in ('canonical', 'prev', 'next', 'alternate') for r in rel):\n",
    "            n = normalize_url(tag['href'], base=base_url)\n",
    "            if n:\n",
    "                found.add(n)\n",
    "\n",
    "    # filter and return\n",
    "    out = set()\n",
    "    for u in found:\n",
    "        if not u:\n",
    "            continue\n",
    "        lu = u.lower()\n",
    "        if lu.startswith('mailto:') or lu.startswith('tel:'):\n",
    "            continue\n",
    "        if any(lu.endswith(ext) for ext in EXTENSION_SKIP):\n",
    "            continue\n",
    "        out.add(u)\n",
    "\n",
    "    logger.info(f'Extracted {len(out)} normalized links from {base_url}')\n",
    "    return out\n",
    "\n",
    "# Optional simple classifier fallback (no OpenAI)\n",
    "def simple_classify(title_or_text):\n",
    "    t = (title_or_text or '').lower()\n",
    "    if 'course' in t or 'enroll' in t or 'training' in t:\n",
    "        return 'course'\n",
    "    if 'certif' in t or 'certificate' in t or 'exam' in t:\n",
    "        return 'certification'\n",
    "    if 'product' in t or 'buy' in t or 'price' in t:\n",
    "        return 'product'\n",
    "    if 'press' in t or 'news' in t or 'announcement' in t:\n",
    "        return 'announcement'\n",
    "    if 'blog' in t or 'case study' in t or 'case-study' in t:\n",
    "        return 'blog'\n",
    "    return 'other'\n",
    "\n",
    "# Main crawl function\n",
    "async def crawl(start_url, output='out.json', max_pages=500, concurrency=3, delay=0.4, ignore_robots=False, use_playwright=True):\n",
    "    logger.info(f'start crawl for {start_url}')\n",
    "    parsed_start = urlparse(start_url)\n",
    "    base_host = parsed_start.netloc\n",
    "    base_root = f'{parsed_start.scheme}://{parsed_start.netloc}'\n",
    "\n",
    "    # robots.txt\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(base_root + '/robots.txt')\n",
    "        rp.read()\n",
    "        logger.debug('robots.txt loaded')\n",
    "    except Exception:\n",
    "        rp = None\n",
    "        logger.debug('robots.txt not available or failed to load')\n",
    "\n",
    "    q = deque([normalize_url(start_url)])\n",
    "    visited = set()\n",
    "    discovered = set(q)\n",
    "    results = []\n",
    "\n",
    "    playwright_ctx = None\n",
    "    browser = None\n",
    "    browser_context = None\n",
    "\n",
    "    # Use async context for playwright if available and requested\n",
    "    use_pw = use_playwright and (async_playwright is not None)\n",
    "    ctx_manager = async_playwright() if use_pw else None\n",
    "\n",
    "    async def start_playwright():\n",
    "        nonlocal playwright_ctx, browser, browser_context\n",
    "        playwright_ctx = await async_playwright().__aenter__()  # will be closed later\n",
    "        browser = await playwright_ctx.chromium.launch(headless=True)\n",
    "        browser_context = await browser.new_context(user_agent=USER_AGENT)\n",
    "        logger.info('Playwright started')\n",
    "\n",
    "    async def stop_playwright():\n",
    "        nonlocal playwright_ctx, browser, browser_context\n",
    "        try:\n",
    "            if browser_context is not None:\n",
    "                logger.info('closing Playwright context')\n",
    "                await browser_context.close()\n",
    "            if browser is not None:\n",
    "                logger.info('closing Playwright browser')\n",
    "                await browser.close()\n",
    "            if playwright_ctx is not None:\n",
    "                await playwright_ctx.__aexit__(None, None, None)\n",
    "                logger.info('Playwright stopped')\n",
    "        except Exception as e:\n",
    "            logger.warning(f'Error during Playwright shutdown: {e}')\n",
    "\n",
    "    if use_pw:\n",
    "        # start playwright upfront\n",
    "        try:\n",
    "            await start_playwright()\n",
    "        except Exception as e:\n",
    "            logger.warning(f'Could not start Playwright: {e}')\n",
    "            use_pw = False\n",
    "\n",
    "    # worker\n",
    "    async def worker(worker_id):\n",
    "        nonlocal q, visited, discovered, results, browser_context, use_pw\n",
    "        logger.info(f'worker {worker_id} started')\n",
    "        while q and len(visited) < max_pages:\n",
    "            url = None\n",
    "            try:\n",
    "                url = q.popleft()\n",
    "            except Exception:\n",
    "                break\n",
    "            if not url:\n",
    "                continue\n",
    "            if url in visited:\n",
    "                continue\n",
    "            # host restriction\n",
    "            if urlparse(url).netloc != base_host:\n",
    "                logger.debug(f'worker {worker_id} skipping external host: {url}')\n",
    "                visited.add(url)\n",
    "                continue\n",
    "            # robots\n",
    "            if rp and not ignore_robots and not rp.can_fetch(USER_AGENT, url):\n",
    "                logger.debug(f'worker {worker_id} blocked by robots: {url}')\n",
    "                visited.add(url)\n",
    "                continue\n",
    "\n",
    "            logger.info(f'worker {worker_id} crawling: {url} (visited {len(visited)+1}/{max_pages})')\n",
    "            html = None\n",
    "            extra_links = []\n",
    "            # try plain fetch\n",
    "            html = await asyncio.to_thread(fetch_plain, url)\n",
    "            used_playwright = False\n",
    "            # if no html or small html and playwright available, use it\n",
    "            if (not html or len(html) < 500) and use_pw and browser_context is not None:\n",
    "                logger.debug(f'worker {worker_id} using Playwright for {url}')\n",
    "                html, extra_links = await fetch_with_playwright(browser_context, url)\n",
    "                used_playwright = True\n",
    "\n",
    "            visited.add(url)\n",
    "            if not html:\n",
    "                logger.warning(f'worker {worker_id} failed to fetch html for {url}')\n",
    "                await asyncio.sleep(delay)\n",
    "                continue\n",
    "\n",
    "            # extract links from HTML\n",
    "            links = extract_links_from_html(url, html)\n",
    "            # add any extra js-collected links from Playwright evaluation\n",
    "            for e in extra_links:\n",
    "                n = normalize_url(e, base=url)\n",
    "                if n:\n",
    "                    links.add(n)\n",
    "\n",
    "            logger.info(f'worker {worker_id} found {len(links)} links on {url}')\n",
    "\n",
    "            # simple content sampling: title/meta\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            title = None\n",
    "            if soup.find('h1') and soup.find('h1').get_text(strip=True):\n",
    "                title = soup.find('h1').get_text(strip=True)\n",
    "            elif soup.title and soup.title.string:\n",
    "                title = soup.title.string.strip()\n",
    "\n",
    "            classification = simple_classify((title or '') + ' ' + (soup.get_text(' ')[:800] or ''))\n",
    "\n",
    "            # store result for this page\n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'classification': classification,\n",
    "                'rendered_with_playwright': used_playwright,\n",
    "                'links_found': sorted(list(links))[:30],  # store up to 30 sample links per page to avoid huge output\n",
    "            })\n",
    "            logger.info(f'worker {worker_id} saved page {url} classified as {classification} (links sample stored)')\n",
    "\n",
    "            # enqueue normalized links\n",
    "            enqueued = 0\n",
    "            for l in links:\n",
    "                if not l:\n",
    "                    continue\n",
    "                # skip static assets\n",
    "                if any(l.lower().endswith(ext) for ext in EXTENSION_SKIP):\n",
    "                    continue\n",
    "                if urlparse(l).netloc == base_host and l not in discovered:\n",
    "                    discovered.add(l)\n",
    "                    q.append(l)\n",
    "                    enqueued += 1\n",
    "            logger.debug(f'worker {worker_id} enqueued {enqueued} new links from {url}')\n",
    "\n",
    "            await asyncio.sleep(delay)\n",
    "\n",
    "        logger.info(f'worker {worker_id} finished')\n",
    "\n",
    "    # start workers\n",
    "    tasks = [asyncio.create_task(worker(i)) for i in range(concurrency)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # shutdown playwright cleanly\n",
    "    if use_pw:\n",
    "        await stop_playwright()\n",
    "\n",
    "    # write output\n",
    "    out = {\n",
    "        'start_url': start_url,\n",
    "        'scraped_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "        'results': results,\n",
    "        'visited_count': len(visited),\n",
    "    }\n",
    "    with open(output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f'crawl complete. visited={len(visited)} saved={output}')\n",
    "\n",
    "# CLI\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='AI-enabled link crawler (improved discovery)')\n",
    "    parser.add_argument('--start-url', required=True)\n",
    "    parser.add_argument('--output', default='netcom_links.json')\n",
    "    parser.add_argument('--max-pages', type=int, default=200)\n",
    "    parser.add_argument('--concurrency', type=int, default=3)\n",
    "    parser.add_argument('--delay', type=float, default=0.4)\n",
    "    parser.add_argument('--ignore-robots', action='store_true')\n",
    "    parser.add_argument('--no-playwright', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    use_playwright = not args.no_playwright and async_playwright is not None\n",
    "    try:\n",
    "        asyncio.run(crawl(args.start_url, output=args.output, max_pages=args.max_pages, concurrency=args.concurrency, delay=args.delay, ignore_robots=args.ignore_robots, use_playwright=use_playwright))\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('Interrupted by user')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8f1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
